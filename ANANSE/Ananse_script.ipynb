{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colonial-compression",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n",
      "2021-05-03 15:27:16,139 - INFO - No config found.\n",
      "2021-05-03 15:27:16,141 - INFO - Creating new config.\n",
      "2021-05-03 15:27:16,172 - INFO - Using included version of MDmodule.\n",
      "2021-05-03 15:27:45,184 - INFO - Using system version of MEME.\n",
      "2021-05-03 15:28:03,943 - INFO - Using system version of MEMEW.\n",
      "2021-05-03 15:28:37,520 - INFO - Using system version of DREME.\n",
      "2021-05-03 15:28:56,648 - INFO - Using system version of Weeder.\n",
      "2021-05-03 15:29:14,700 - INFO - Using system version of GADEM.\n",
      "2021-05-03 15:29:14,702 - INFO - Using included version of MotifSampler.\n",
      "2021-05-03 15:29:32,678 - INFO - Using system version of Trawler.\n",
      "2021-05-03 15:29:32,680 - INFO - Using included version of Improbizer.\n",
      "2021-05-03 15:29:32,681 - INFO - Using included version of BioProspector.\n",
      "2021-05-03 15:29:32,683 - INFO - Using included version of Posmo.\n",
      "2021-05-03 15:29:32,684 - INFO - Using included version of ChIPMunk.\n",
      "2021-05-03 15:29:32,685 - INFO - Using included version of AMD.\n",
      "2021-05-03 15:29:32,687 - INFO - Using included version of HMS.\n",
      "2021-05-03 15:29:50,926 - INFO - Using system version of Homer.\n",
      "2021-05-03 15:30:08,617 - INFO - Using system version of XXmotif.\n",
      "2021-05-03 15:30:26,642 - INFO - Using system version of ProSampler.\n",
      "2021-05-03 15:30:26,644 - WARNING - Yamda not in config\n",
      "2021-05-03 15:30:45,476 - INFO - Using system version of DiNAMO.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from gimmemotifs.preprocessing import combine_peaks\n",
    "import subprocess as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collective-while",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the table to convert the chromosome names to the USCS format\n",
    "CRCH38_ensembl2UCSC = pd.read_csv('/scratch/bacint/jdeleuw/KC_ATAC/analysis/bigBed/CRCH38_ensembl2UCSC.txt', sep ='\\t',header = 0, index_col=False)\n",
    "\n",
    "#make dictionary which can be used to convert ensembl to UCSC format \n",
    "ensembl_to_UCSC = {}\n",
    "UCSC_to_ensembl = {}\n",
    "for row in CRCH38_ensembl2UCSC.itertuples():\n",
    "    ensembl_to_UCSC[row[1]] = row[2]\n",
    "    UCSC_to_ensembl[row[2]] = row[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "turned-conclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = pd.read_csv('/scratch/bacint/jdeleuw/ananse/ananse_paths.csv', sep = ',', header = 0, index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deluxe-colombia",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspended-rapid",
   "metadata": {},
   "source": [
    "# Ananse enhancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "median-rendering",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Copy narrowPeak files to the ananse map and change the name to the day of differentiation\n",
    "for narrowPeak, output_dir in zip(paths['narrowPeak'], paths['narrowPeak_descriptive_name']):\n",
    "    sp.check_call(f'cp {narrowPeak} '\n",
    "                  f'{output_dir}',\n",
    "                  shell = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "minus-update",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy bigWig files to the ananse map and change the name to the day of differentiation\n",
    "for bigWig, output_dir in zip(paths['bigWig'], paths['bigWig_descriptive_name']):\n",
    "    sp.check_call(f'cp {bigWig} '\n",
    "                  f'{output_dir}',\n",
    "                  shell = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "optimum-outside",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select the overlapping 200 bp peaks\n",
    "genome_path_size = '/scratch/bacint/jdeleuw/genomes/hg38/hg38.fa.sizes'\n",
    "\n",
    "peak_files = []\n",
    "\n",
    "for index, cell_type in paths.iterrows():\n",
    "    peak_files.extend(paths.narrowPeak_descriptive_name[index].split(' '))\n",
    "\n",
    "all_peaks_merged = combine_peaks(peak_files,\n",
    "                                 genome_path_size,\n",
    "                                 int(200),\n",
    "                                 True)\n",
    "#extend each summit to 200bp\n",
    "all_peaks_merged.start = all_peaks_merged.start.astype(int) - 100\n",
    "all_peaks_merged.end = all_peaks_merged.end.astype(int) + 100\n",
    "\n",
    "all_peaks_merged = all_peaks_merged[~all_peaks_merged.chrom.str.contains(\"GL|Un|KI|MT|X|Y\")]\n",
    "\n",
    "all_peaks_merged.to_csv(\n",
    "    '/scratch/bacint/jdeleuw/ananse/enhancer_data/peaks_merged.tsv',\n",
    "    sep=\"\\t\",\n",
    "    header=False,\n",
    "    index=False,\n",
    ")\n",
    "\n",
    "all_peaks_merged['peak'] = all_peaks_merged.apply(lambda row: ' '.join(row.values.astype(str)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historic-radiation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify bigWig paths\n",
    "bigWig_paths = []\n",
    "bigWig_paths.extend(paths['bigWig_descriptive_name'])\n",
    "\n",
    "#specify output directory\n",
    "output_enhancer_activity = []\n",
    "output_enhancer_activity.extend(paths['bigWigSummary_output'].dropna())\n",
    "output_dir_slice = 0\n",
    "\n",
    "#Calculate the max score for every bw file\n",
    "for index, bigWig in zip(paths.index, bigWig_paths):  \n",
    "    #for duplicate A, which should not be written to a file yet\n",
    "    if index.endswith('A') or index.endswith('2'):\n",
    "        output = []\n",
    "        for line in all_peaks_merged['peak']:\n",
    "            output_bigWigSummary = sp.run(f'bigWigSummary '\n",
    "                                          f'{bigWig} '\n",
    "                                          f'-type=max '\n",
    "                                          f'{line} 1',\n",
    "                                          stdout = sp.PIPE,\n",
    "                                          shell = True) \n",
    "            max_score = str(output_bigWigSummary.stdout).strip(\"\\n'\")[2:-2].strip()\n",
    "            row = line.replace(' ','\\t') + '\\t' + max_score\n",
    "            output.append(row)    \n",
    "        peak_intensity_temp = pd.DataFrame(output)\n",
    "        #split the row in the rows chr, start, end and intensity\n",
    "        peak_intensity = peak_intensity_temp.loc[:,0].str.split('\\t', expand = True)\n",
    "        peak_intensity = peak_intensity.rename(columns = {0:'chrom', 1:'start', 2:'end', 3:'intensity_A'})\n",
    "\n",
    "        #convert the data type of start and end to float, so it can be properly sorted\n",
    "        cols =  ['start', 'end', 'intensity_A']\n",
    "        peak_intensity[cols] = peak_intensity[cols].apply(pd.to_numeric, errors='coerce')\n",
    "        sorted_peak_intensity_A = peak_intensity.sort_values(by = ['chrom', 'start', 'end'])\n",
    "\n",
    "    #for duplicate B, for which the average will be calculated with duplicate A and written to a file\n",
    "    else:\n",
    "        output = []\n",
    "        for line in all_peaks_merged['peak']:\n",
    "            output_bigWigSummary = sp.run(f'bigWigSummary '\n",
    "                                          f'{bigWig} '\n",
    "                                          f'-type=max '\n",
    "                                          f'{line} 1',\n",
    "                                          stdout = sp.PIPE,\n",
    "                                          shell = True) \n",
    "            max_score = str(output_bigWigSummary.stdout).strip(\"\\n'\")[2:-2].strip()\n",
    "            row = line.replace(' ','\\t') + '\\t' + max_score\n",
    "            output.append(row)    \n",
    "        peak_intensity_temp = pd.DataFrame(output)\n",
    "        #split the row in the rows chr, start, end and intensity\n",
    "        peak_intensity = peak_intensity_temp.loc[:,0].str.split('\\t', expand = True)\n",
    "        peak_intensity = peak_intensity.rename(columns = {0:'chrom', 1:'start', 2:'end', 3:'intensity_B'})\n",
    "\n",
    "        #convert the data type of start and end to float, so it can be properly sorted\n",
    "        cols =  ['start', 'end', 'intensity_B']\n",
    "        peak_intensity[cols] = peak_intensity[cols].apply(pd.to_numeric, errors='coerce')\n",
    "        sorted_peak_intensity_B = peak_intensity.sort_values(by = ['chrom', 'start', 'end'])\n",
    "\n",
    "        #calculate average of max score between two duplicates\n",
    "        sorted_peak_intensity_A['intensity_B'] = sorted_peak_intensity_B['intensity_B']\n",
    "        sorted_peak_intensity_A['intensity_avg'] = sorted_peak_intensity_A[['intensity_A', 'intensity_B']].mean(axis = 1).round()\n",
    "        avg_enhancer_intensity = sorted_peak_intensity_A.drop(sorted_peak_intensity_A[['intensity_A', 'intensity_B']], axis=1)\n",
    "\n",
    "        avg_enhancer_intensity.to_csv(output_enhancer_activity[output_dir_slice], sep = '\\t', header = False, index = False)\n",
    "        output_dir_slice += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "potential-performance",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort the enhancer intensity files\n",
    "#specify input enhancer activity paths\n",
    "enhancer_activity = []\n",
    "enhancer_activity.extend(paths['bigWigSummary_output'].dropna())\n",
    "\n",
    "#specify output directory\n",
    "output_paths = []\n",
    "output_paths.extend(paths['bed_sorted'].dropna())\n",
    "\n",
    "for bed_file, output_dir in zip(enhancer_activity, output_paths):\n",
    "    sp.check_call(f'LC_COLLATE=C sort -k1,1 -k2,2n '\n",
    "                  f'{bed_file} '\n",
    "                  f'> {output_dir}',\n",
    "                  shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binding-fitness",
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete the config file of gimmemotifs\n",
    "sp.check_call(f'rm -r '\n",
    "              f'/mbshome/jdeleuw/.config/gimmemotifs/',\n",
    "              shell = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mighty-serum",
   "metadata": {},
   "source": [
    "# Ananse binding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sporting-patient",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run ananse binding\n",
    "#specify the enhancer intensity paths constructed in the last step\n",
    "enhancer_intensity = []\n",
    "enhancer_intensity.extend(paths['bed_sorted'].dropna())\n",
    "\n",
    "#specify the output directory paths\n",
    "output_paths = []\n",
    "output_paths.extend(paths['binding_output'].dropna())\n",
    "\n",
    "genome = '/scratch/bacint/jdeleuw/genomes/hg38/hg38.fa'\n",
    "\n",
    "#specify the log output dir\n",
    "binding_log = []\n",
    "binding_log.extend(paths['binding_log'].dropna())\n",
    "\n",
    "for bed, output_dir, log in zip(enhancer_intensity, output_paths, binding_log):\n",
    "    sp.check_call(f'nice -15 '\n",
    "                  f'ananse binding '\n",
    "                  f'-r {bed} '\n",
    "                  f'-o {output_dir} '\n",
    "                  f'-g {genome} '\n",
    "                  f'-t ATAC '\n",
    "                  f'2> {log}',\n",
    "                  shell = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ambient-bibliography",
   "metadata": {},
   "source": [
    "# Ananse network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "talented-brunei",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run ananse network\n",
    "#specify TPM_paths\n",
    "TPM_duplicate_A = []\n",
    "TPM_duplicate_B = []\n",
    "\n",
    "for index, values in paths.iterrows():\n",
    "    if index.endswith('A') or index.endswith('KC2'):\n",
    "        TPM_duplicate_A.append(values.TPM)\n",
    "    else:\n",
    "        TPM_duplicate_B.append(values.TPM)\n",
    "#specify binding paths\n",
    "binding_paths = []\n",
    "binding_paths.extend(paths['binding_output'].dropna())\n",
    "#specify network output\n",
    "network_paths = []\n",
    "network_paths.extend(paths.network_output.dropna())\n",
    "#specify log output\n",
    "network_log = []\n",
    "network_log.extend(paths.network_log.dropna())     \n",
    "\n",
    "bed_file_network = '/scratch/bacint/jdeleuw/ananse/genome/hg38_genes.bed'\n",
    "genome = '/scratch/bacint/jdeleuw/genomes/hg38/hg38.fa'\n",
    "\n",
    "for TPM_A, TPM_B, binding, output_dir, log in zip(TPM_duplicate_A, TPM_duplicate_B,\n",
    "                                                  binding_paths,\n",
    "                                                  network_paths,\n",
    "                                                  network_log):\n",
    "    sp.check_call(f'nice -15 ananse network '\n",
    "                  f'-e {TPM_A} {TPM_B} '\n",
    "                  f'-b {binding} '\n",
    "                  f'-o {output_dir} '\n",
    "                  f'-a {bed_file_network} '\n",
    "                  f'-g {genome} '\n",
    "                  f'2> {log}',\n",
    "                  shell = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "polish-supply",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = pd.read_csv('/scratch/bacint/jdeleuw/ananse/ananse_paths.csv', sep = ',', header = 0, index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "known-qualification",
   "metadata": {},
   "source": [
    "# Ananse influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-saying",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run ananse influence\n",
    "#Specify DEgenes paths\n",
    "DEgenes_paths = []\n",
    "DEgenes_paths.extend(paths.Deseq.dropna())\n",
    "#Specify the network input paths\n",
    "network_paths = []\n",
    "network_paths.extend(paths.network_output.dropna())\n",
    "#Specify influence output\n",
    "influence_paths = []\n",
    "influence_paths.extend(paths.influence_output.dropna())\n",
    "#Specify log output\n",
    "influence_log = []\n",
    "influence_log.extend(paths.influence_log.dropna())\n",
    "\n",
    "for network, DEgenes, output_dir, log in zip(network_paths[1:], \n",
    "                                             DEgenes_paths, \n",
    "                                             influence_paths, \n",
    "                                             influence_log):\n",
    "    sp.check_call(f'nice -15 ananse influence '\n",
    "                  f'-s /scratch/bacint/jdeleuw/ananse/network_data/Day0_network.txt '\n",
    "                  f'-t {network} '\n",
    "                  f'-d {DEgenes} '\n",
    "                  f'-p '\n",
    "                  f'-o {output_dir} ' \n",
    "                  f'2> {log}',\n",
    "                  shell = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "employed-dealing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run ananse influence with day 0 as the target\n",
    "#Specify DEgenes paths\n",
    "rev_DEgenes_paths = []\n",
    "rev_DEgenes_paths.extend(paths.reverse_Deseq.dropna())\n",
    "#Specify the network input paths\n",
    "network_paths = []\n",
    "network_paths.extend(paths.network_output.dropna())\n",
    "#Specify influence output\n",
    "rev_influence_paths = []\n",
    "rev_influence_paths.extend(paths.reverse_influence_output.dropna())\n",
    "#Specify log output\n",
    "rev_influence_log = []\n",
    "rev_influence_log.extend(paths.reverse_influence_log.dropna())\n",
    "\n",
    "for network, DEgenes, output_dir, log in zip(network_paths[1:], \n",
    "                                             rev_DEgenes_paths, \n",
    "                                             rev_influence_paths, \n",
    "                                             rev_influence_log):\n",
    "    sp.check_call(f'nice -15 ananse influence '\n",
    "                  f'-s {network} '\n",
    "                  f'-t /scratch/bacint/jdeleuw/ananse/network_data/Day0_network.txt '\n",
    "                  f'-d {DEgenes} '\n",
    "                  f'-p '\n",
    "                  f'-o {output_dir} ' \n",
    "                  f'2> {log}',\n",
    "                  shell = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "advanced-singapore",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run ananse influence in chronological order\n",
    "#Specify DEgenes paths\n",
    "chron_DEgenes_paths = []\n",
    "chron_DEgenes_paths.extend(paths.chron_Deseq.dropna())\n",
    "#Specify the network input paths\n",
    "network_paths = []\n",
    "network_paths.extend(paths.network_output.dropna())\n",
    "#Specify influence output\n",
    "chron_influence_paths = []\n",
    "chron_influence_paths.extend(paths.chron_influence_output.dropna())\n",
    "#Specify log output\n",
    "chron_influence_log = []\n",
    "chron_influence_log.extend(paths.chron_influence_log.dropna())\n",
    "\n",
    "for source_network, target_network, DEgenes, output_dir, log in zip(network_paths,\n",
    "                                                                    network_paths[1:], \n",
    "                                                                    chron_DEgenes_paths, \n",
    "                                                                    chron_influence_paths, \n",
    "                                                                    chron_influence_log):\n",
    "    sp.check_call(f'nice -15 ananse influence '\n",
    "                  f'-s {source_network} '\n",
    "                  f'-t {target_network} '\n",
    "                  f'-d {DEgenes} '\n",
    "                  f'-p '\n",
    "                  f'-o {output_dir} ' \n",
    "                  f'2> {log}',\n",
    "                  shell = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "under-asian",
   "metadata": {},
   "source": [
    "# Average comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "automotive-footwear",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Calculate the enhancer activity\n",
    "#specify paths to enhancer activity files\n",
    "enhancer_activity_paths = []\n",
    "enhancer_activity_paths.extend(paths['bigWigSummary_output'].dropna())\n",
    "#load enhancer activity files\n",
    "variable_names = [day + '_enhancer' for day in paths.index]\n",
    "for path, name in zip(enhancer_activity_paths, variable_names):\n",
    "    globals()[name] = pd.read_csv(path, sep ='\\t', header = None, index_col=False)\n",
    "\n",
    "#calculate avg enhancer intensity\n",
    "avg_intensity = (Day0_enhancer.loc[:,3] + Day07_enhancer.loc[:,3] + \n",
    "                 Day14_enhancer.loc[:,3] + Day21_enhancer.loc[:,3] + \n",
    "                 Day43_enhancer.loc[:,3] + Day60_enhancer.loc[:,3] + \n",
    "                 KC_enhancer.loc[:,3]) // 7\n",
    "\n",
    "#merge the enhancer activities with the peak location\n",
    "cols = [0,1,2]\n",
    "average_bed = Day0_enhancer[cols].join(avg_intensity)\n",
    "average_bed.to_csv('/scratch/bacint/jdeleuw/ananse/enhancer_data/enhancer_intensity/average_enhancer_intensity.tsv', sep = '\\t', header = False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "round-business",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run ananse binding\n",
    "sp.check_call(f'nice -15 '\n",
    "              f'ananse binding '\n",
    "              f'-r /scratch/bacint/jdeleuw/ananse/enhancer_data/enhancer_intensity_sorted/average_enhancer_activity.tsv '\n",
    "              f'-o /scratch/bacint/jdeleuw/ananse/binding_data/binding_avg.txt '\n",
    "              f'-g {genome} '\n",
    "              f'-t ATAC '\n",
    "              f'2> /scratch/bacint/jdeleuw/ananse/logs/binding_avg.log',\n",
    "              shell = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "removed-philosophy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate average TPM\n",
    "variable_names = [day + '_TPM' for day in paths.index]\n",
    "for path, name in zip(paths.TPM, variable_names):\n",
    "    globals()[name] = pd.read_csv(path, sep ='\\t', header = 0, index_col=False)\n",
    "    \n",
    "avg_tpm = (Day0A_TPM.iloc[:,1] + Day0B_TPM.iloc[:,1] + \n",
    "           Day07A_TPM.iloc[:,1] + Day07B_TPM.iloc[:,1] +\n",
    "           Day14A_TPM.iloc[:,1] + Day14B_TPM.iloc[:,1] +\n",
    "           Day21A_TPM.iloc[:,1] + Day21B_TPM.iloc[:,1] +\n",
    "           Day43A_TPM.iloc[:,1] + Day43B_TPM.iloc[:,1] +\n",
    "           Day60A_TPM.iloc[:,1] + Day60B_TPM.iloc[:,1] +\n",
    "           KC2_TPM.iloc[:,1] +  KC3_TPM.iloc[:,1]) // 14\n",
    "average_TPM = Day0A_TPM.drop(['tpm'], axis=1).join(avg_tpm)\n",
    "average_TPM.to_csv('/scratch/bacint/jdeleuw/ananse/expression_data/TPM/avg_TPM.tsv', sep = '\\t', header = True, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulation-atlas",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run ananse network\n",
    "bed_file_network = '/scratch/bacint/jdeleuw/ananse/genome/hg38_genes.bed'\n",
    "genome = '/scratch/bacint/jdeleuw/genomes/hg38/hg38.fa'\n",
    "\n",
    "sp.check_call(f'nice -15 ananse network '\n",
    "              f'-e /scratch/bacint/jdeleuw/ananse/expression_data/TPM/avg_TPM.tsv '\n",
    "              f'-b /scratch/bacint/jdeleuw/ananse/binding_data/binding_avg.txt '\n",
    "              f'-o /scratch/bacint/jdeleuw/ananse/network_data/avg_network.txt '\n",
    "              f'-a {bed_file_network} '\n",
    "              f'-g {genome} '\n",
    "              f'2> /scratch/bacint/jdeleuw/ananse/logs/avg_network.log ',\n",
    "              shell = True\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blind-bowling",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run ananse influence by comparing everything to the average\n",
    "#Specify the network input paths\n",
    "network_paths = []\n",
    "network_paths.extend(paths.network_output.dropna())\n",
    "#Specify influence output\n",
    "influence_paths = []\n",
    "influence_paths.extend(paths.avg_influence_output.dropna())\n",
    "#Specify log output\n",
    "influence_log = []\n",
    "influence_log.extend(paths.avg_influence_log.dropna())\n",
    "\n",
    "for network, output_dir, log in zip(network_paths, influence_paths, influence_log):\n",
    "    sp.check_call(f'nice -15 ananse influence '\n",
    "                  f'-s /scratch/bacint/jdeleuw/ananse/network_data/avg_network.txt '\n",
    "                  f'-t {network} '\n",
    "                  f'-d /scratch/bacint/jdeleuw/ananse/expression_data/DESeq/DEgenes_Day0vsAdultKC.tsv '\n",
    "                  f'-p '\n",
    "                  f'-o {output_dir} ' \n",
    "                  f'2> {log}',\n",
    "                  shell = True) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
